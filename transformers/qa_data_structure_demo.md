# 抽取式问答数据预处理演示

## 概述

本文档展示了 `prepare_train_features` 方法如何将原始问答数据转换为模型训练所需的格式。

## 数据转换流程

### 1. 原始数据格式

```python
raw_data = {
    "question": ["  什么是人工智能？"],  # 注意前面的空格
    "context": [
        "人工智能（AI）是计算机科学的一个分支，它企图了解智能的实质，并生产出一种新的能以人类智能相似的方式做出反应的智能机器。"
    ],
    "answers": {
        "text": ["人工智能"],           # 答案文本
        "answer_start": [0]            # 答案在上下文中的起始位置
    }
}
```

### 2. 处理步骤详解

#### 步骤1: 文本清理
- **输入**: `"  什么是人工智能？"` (带前导空格)
- **输出**: `"什么是人工智能？"` (清理后)
- **作用**: 移除不必要的空白字符，避免token浪费

#### 步骤2: 标记化处理
- **问题tokens**: `['什', '么', '是', '人', '工', '智', '能', '？']`
- **上下文tokens**: `['人', '工', '智', '能', '（', 'A', 'I', '）', '是', '计', ...]`
- **组合后**: 问题和上下文拼接，形成完整的输入序列

#### 步骤3: 答案位置计算
- **原始答案位置**: 字符级别，在上下文中的第0个字符开始
- **转换后位置**: token级别，在token序列中的第8-12个位置
- **计算逻辑**: 
  - 答案起始token位置 = 问题长度 + 答案在原文中的起始位置
  - 答案结束token位置 = 答案起始位置 + 答案长度

#### 步骤4: 最终数据结构

```python
processed_data = {
    "input_ids": ['什', '么', '是', '人', '工', '智', '能', '？', '人', '工', '智', '能', ...],
    "attention_mask": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...],
    "start_positions": [8],    # 答案起始token位置
    "end_positions": [12]      # 答案结束token位置
}
```

## 关键特性

### 1. 位置映射
- **字符位置 → Token位置**: 将人类可读的字符位置转换为模型可处理的token位置
- **精确标注**: 为模型提供答案的确切边界信息

### 2. 长文本处理
- **截断策略**: 当文本超过最大长度时，智能截断
- **步幅处理**: 对于超长文档，使用滑动窗口生成多个训练样本

### 3. 边界情况处理
- **无答案情况**: 使用特殊token（如CLS）标记
- **答案超出范围**: 当答案不在当前文本片段内时的处理策略

## 实际应用示例

### 示例1: 简单问答
```
问题: "什么是人工智能？"
上下文: "人工智能（AI）是计算机科学的一个分支..."
答案: "人工智能"
结果: ✅ 成功匹配
```

### 示例2: 复杂答案
```
问题: "AI的主要应用领域有哪些？"
上下文: "人工智能的主要应用领域包括机器人、语言识别、图像识别、自然语言处理和专家系统等。"
答案: "机器人、语言识别、图像识别、自然语言处理和专家系统"
结果: ⚠️ 部分匹配（由于截断）
```

### 示例3: 精确匹配
```
问题: "机器学习的定义是什么？"
上下文: "机器学习是人工智能的一个子领域..."
答案: "机器学习"
结果: ✅ 成功匹配
```

## 训练效果

通过这种数据预处理，模型能够：

1. **学习答案定位**: 理解如何从上下文中找到答案的确切位置
2. **理解问题-上下文关系**: 学会将问题与相关上下文片段关联
3. **处理各种情况**: 适应不同长度、不同位置的答案
4. **提高准确性**: 通过精确的位置标注提高答案抽取的准确性

## 总结

`prepare_train_features` 方法是抽取式问答任务成功的关键预处理步骤，它将原始文本数据转换为结构化的训练数据，为模型提供了学习答案抽取能力所需的监督信号。 